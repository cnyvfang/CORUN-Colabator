name: trainS2_CORUN_RIDCP500
model_type: Colabator_with_Transmission
scale: 1
num_gpu: auto  # auto: can infer from your visible devices automatically. official: 8 GPUs
manual_seed: 0

colabator:
  use_clip: true
  use_nr_iqa: true
  clip_model_type: daclip_ViT-B-32 # clip model name in openclip
  tokenizer_type: ViT-B-32
  pretrained_clip_weight: ./pretrained_weights/daclip_ViT-B-32.pt
  degradation_type: [1]
  # for daclip
  # 0: 'motion-blurry', 1: 'hazy', 2: 'jpeg-compressed', 3: 'low-light',
  # 4: 'noisy', 5: 'raindrop', 6: 'rainy', 7: 'shadowed', 8: 'snowy', 9: 'uncompleted'
  clip_better: lower

  nr_iqa_type: musiq # you can choose any nr_iqa method from pyiqa model card.
  nr_iqa_better: higher # higher, lower
  nr_iqa_scale: [0,100] # scale or sigmoid

  block_size: 32 # trusted weight block size

  weight_map_calculation: 'addition' # addition, multiplication

# dataset and data loader settings
datasets:
  train:
    name: TrainSet
    type: SemiHazeOnlineDataset
    dataroot_gt: /data/fcy/Datasets/Dehaze/RIDCP/rgb_500
    dataroot_depth: /data/fcy/Datasets/Dehaze/RIDCP/depth_500
    dataroot_real: /data/fcy/Datasets/Dehaze/UnannotatedHazyImages-cleaned
    beta_range: [ 0.2, 2.8 ]
    A_range: [ 0.25, 1.8 ]
    color_p: 1.0
    color_range: [ -0.025, 0.025 ]
    use_resize_crop: true
    use_flip: true
    use_rot: false
    filename_tmpl: '{}'
    io_backend:
      type: disk

    num_worker_per_gpu: 20
    batch_size_per_gpu: 16
    mini_batch_sizes: [ 1 ]             # Batch size per gpu
    iters: [ 50000 ]
    gt_size: 288   # Max patch size for progressive training
    gt_sizes: [ 288 ]  # Patch sizes for progressive training.

    # data loader
    use_shuffle: true
    dataset_enlarge_ratio: 50
    prefetch_mode: ~


  # Uncomment these for validation
  val:
    name: RTTS
    type: SingleDataset
    dataroot_lq: /data/fcy/Datasets/Dehaze/RTTS
    rescale_too_large_image: false
    io_backend:
      type: disk
# network structures

###########################################
# this is a sample
# you can modify this part for your network
network_g:
  type: CORUN
  depth: 4
###########################################

# path
path:
  pretrain_network_g: ./experiments/trainS1_CORUN_RIDCP500/models/net_g.pth # pretrained weight
  pretrain_network_memory_bank: ~
  param_key_g: params_ema
  strict_load_g: true
  resume_state: ~

# training settings
train:
  ema_decay: 0.999

  optim_g:
    type: AdamW
    lr: !!float 5e-5
    weight_decay: 0
    betas: [0.9, 0.999]


  gen_scheduler:
    type: CosineAnnealingRestartCyclicLR
    periods: [50000]
    restart_weights: [1]
    eta_mins: [0.000001]


  total_iter: 5000
  warmup_iter: -1  # no warm up

  mixing_augs:
    mixup: false
    mixup_beta: 1.2
    use_identity: true

  ###########################################
  # this is a sample
  # you can modify this part for your losses

  # losses
  pixel_opt:
    type: L1Loss
    loss_weight: !!float 0.5
    reduction: none

  use_clip_loss: true

  contrastperceptual_opt:
    type: ContrastPerceptualLoss
    # vgg_layer_indices: 2,7,12,21,30
    # weights: 1/32, 1/16, 1/8, 1/4, 1
    layer_weights:
      'relu1_1': !!float 3.125e-2
      'relu2_1': !!float 6.25e-2
      'relu3_1': !!float 0.125
      'relu4_1': !!float 0.25
      'relu5_1': !!float 1.0
    vgg_type: vgg19
    use_input_norm: false # keep in [0,1] range
    range_norm: false
    perceptual_weight: !!float 0.2
    style_weight: 0
    criterion: l1

  ###########################################

val:
  pbar: true
  val_freq: !!float 5000
  save_img: true
  window_size: 16
  save_source: true

  metrics:
    brisque:
      type: calculate_brisque
      crop_border: 0
      better: lower
    nima:
      type: calculate_nima
      crop_border: 0
      better: higher

#    only enable when test gt image set is exist
#    ssim:
#      type: calculate_ssim
#      crop_border: 0
#      better: higher
#    psnr:
#      type: calculate_psnr
#      crop_border: 0
#      better: higher


# logging settings
logger:
  print_freq: 500
  save_checkpoint_freq: !!float 5e3
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~

# dist training settings
dist_params:
  backend: nccl
  port: 29500